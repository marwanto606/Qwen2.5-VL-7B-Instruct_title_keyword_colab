{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "# Download model files\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/raw/main/chat_template.json -d /content/Qwen2.5-VL-7B-Instruct -o chat_template.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/raw/main/config.json -d /content/Qwen2.5-VL-7B-Instruct -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/raw/main/generation_config.json -d /content/Qwen2.5-VL-7B-Instruct -o generation_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/raw/main/merges.txt -d /content/Qwen2.5-VL-7B-Instruct -o merges.txt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/model-00001-of-00005.safetensors -d /content/Qwen2.5-VL-7B-Instruct -o model-00001-of-00005.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/model-00002-of-00005.safetensors -d /content/Qwen2.5-VL-7B-Instruct -o model-00002-of-00005.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/model-00003-of-00005.safetensors -d /content/Qwen2.5-VL-7B-Instruct -o model-00003-of-00005.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/model-00004-of-00005.safetensors -d /content/Qwen2.5-VL-7B-Instruct -o model-00004-of-00005.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/model-00005-of-00005.safetensors -d /content/Qwen2.5-VL-7B-Instruct -o model-00005-of-00005.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/raw/main/model.safetensors.index.json -d /content/Qwen2.5-VL-7B-Instruct -o model.safetensors.index.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/raw/main/preprocessor_config.json -d /content/Qwen2.5-VL-7B-Instruct -o preprocessor_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/raw/main/tokenizer.json -d /content/Qwen2.5-VL-7B-Instruct -o tokenizer.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/raw/main/tokenizer_config.json -d /content/Qwen2.5-VL-7B-Instruct -o tokenizer_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/raw/main/vocab.json -d /content/Qwen2.5-VL-7B-Instruct -o vocab.json\n",
        "\n",
        "# Install required Python packages\n",
        "!pip install git+https://github.com/huggingface/transformers accelerate transformers-stream-generator==0.0.5 gradio==4.44.1 qwen-vl-utils pydantic bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import re\n",
        "import torch\n",
        "import threading\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, TextIteratorStreamer, BitsAndBytesConfig\n",
        "import logging\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "checkpoint_path = \"/content/Qwen2.5-VL-7B-Instruct\"\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    checkpoint_path,\n",
        "    quantization_config=quant_config,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "processor = AutoProcessor.from_pretrained(checkpoint_path,trust_remote_code=True,use_fast=True)\n",
        "\n",
        "def _gc():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def _transform_messages(original_messages):\n",
        "    transformed_messages = []\n",
        "    for message in original_messages:\n",
        "        new_content = []\n",
        "        for item in message['content']:\n",
        "            if 'image' in item and isinstance(item['image'], Image.Image):\n",
        "                new_item = {'type': 'image', 'image': item['image']}\n",
        "            elif 'text' in item and isinstance(item['text'], str):\n",
        "                new_item = {'type': 'text', 'text': item['text']}\n",
        "            else:\n",
        "                continue\n",
        "            new_content.append(new_item)\n",
        "        transformed_messages.append({'role': message['role'], 'content': new_content})\n",
        "    return transformed_messages\n",
        "\n",
        "def call_local_model(model, processor, messages):\n",
        "    messages = _transform_messages(messages)\n",
        "    if not messages:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Buat prompt chat dengan template yang disediakan oleh processor\n",
        "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        image_inputs, video_inputs = process_vision_info(messages)\n",
        "        if not image_inputs:\n",
        "            return\n",
        "\n",
        "        if video_inputs is not None:\n",
        "            inputs = processor(\n",
        "                text=[text],\n",
        "                images=image_inputs,\n",
        "                videos=video_inputs,\n",
        "                padding=True,\n",
        "                return_tensors='pt'\n",
        "            ).to(model.device)\n",
        "        else:\n",
        "            inputs = processor(\n",
        "                text=[text],\n",
        "                images=image_inputs,\n",
        "                padding=True,\n",
        "                return_tensors='pt'\n",
        "            ).to(model.device)\n",
        "\n",
        "        tokenizer = processor.tokenizer\n",
        "        streamer = TextIteratorStreamer(tokenizer, timeout=20.0, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "        gen_kwargs = {\n",
        "            'max_new_tokens': 512,\n",
        "            'streamer': streamer,\n",
        "            **inputs\n",
        "        }\n",
        "\n",
        "        def generate():\n",
        "            with torch.no_grad():\n",
        "                try:\n",
        "                    model.generate(**gen_kwargs)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during generation: {str(e)}\")\n",
        "                    torch.cuda.empty_cache()\n",
        "                    raise\n",
        "\n",
        "        generation_thread = threading.Thread(target=generate)\n",
        "        generation_thread.start()\n",
        "\n",
        "        generated_text = ''\n",
        "        for new_text in streamer:\n",
        "            generated_text += new_text\n",
        "            yield generated_text\n",
        "\n",
        "        generation_thread.join()\n",
        "        _gc()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in call_local_model: {str(e)}\")\n",
        "        _gc()\n",
        "        return\n",
        "\n",
        "def generate_image_keywords(image: Image.Image):\n",
        "    try:\n",
        "        if not isinstance(image, Image.Image):\n",
        "            return \"Error: Invalid image input\", \"\"\n",
        "\n",
        "        # Ubah prompt agar output eksplisit dengan format:\n",
        "        prompt_text = (\n",
        "            \"Please analyze the image and generate exactly two lines of output in the following format:\\n\"\n",
        "            \"Title: <A descriptive title of about 10-15 words>\\n\"\n",
        "            \"Keywords: <Exactly 50 relevant keywords or short phrases, separated by commas>\\n\"\n",
        "            \"Focus on visual elements, mood, style, and subject matter.\"\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': [\n",
        "                    {'type': 'image', 'image': image},\n",
        "                    {'type': 'text', 'text': prompt_text}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Kumpulkan respons dari model secara streaming\n",
        "        responses = list(call_local_model(model, processor, messages))\n",
        "        if not responses:\n",
        "            return \"Error: No response from model\", \"\"\n",
        "\n",
        "        # Gunakan respons terakhir untuk diparsing\n",
        "        text = responses[-1].strip()\n",
        "        print(\"Raw model output:\", text)  # Debug output\n",
        "\n",
        "        # Parsing hasil generasi model menggunakan regex\n",
        "        title = \"No title generated\"\n",
        "        keywords = \"No keywords generated\"\n",
        "\n",
        "        # Regex untuk mencari title dan keywords (lebih fleksibel)\n",
        "        title_match = re.search(r\"(?i)title:\\s*\\\"?(.*?)\\\"?$\", text, re.MULTILINE)\n",
        "        keywords_match = re.search(r\"(?i)keywords:\\s*(.*)\", text, re.MULTILINE)\n",
        "\n",
        "        # Ambil title jika ditemukan\n",
        "        if title_match:\n",
        "            title_candidate = title_match.group(1).strip()\n",
        "            if len(title_candidate.split()) >= 2:  # Validasi minimal 2 kata\n",
        "                title = title_candidate\n",
        "            elif title_candidate:  # Fallback jika hanya 1 kata\n",
        "                title = title_candidate\n",
        "\n",
        "        # Ambil keywords jika ditemukan\n",
        "        if keywords_match:\n",
        "            keywords_candidate = keywords_match.group(1).strip().strip('\"')\n",
        "            keyword_list = [k.strip() for k in keywords_candidate.split(\",\") if k.strip()]\n",
        "            if keyword_list:\n",
        "                keywords = \", \".join(keyword_list[:50])  # Batasi 50 keyword maksimal\n",
        "\n",
        "        _gc()  # Garbage collector untuk efisiensi memori\n",
        "        return title, keywords\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generate_image_keywords: {str(e)}\")\n",
        "        return f\"Error processing image: {str(e)}\", \"\"\n",
        "\n",
        "copy_js = \"\"\"\n",
        "function copyToClipboard(text) {\n",
        "    if (navigator.clipboard) {\n",
        "        navigator.clipboard.writeText(text);\n",
        "        const notification = document.createElement(\"div\");\n",
        "        notification.innerText = \"Copied to clipboard!\";\n",
        "        notification.style.position = \"fixed\";\n",
        "        notification.style.top = \"20px\";\n",
        "        notification.style.right = \"20px\";\n",
        "        notification.style.backgroundColor = \"#4caf50\";\n",
        "        notification.style.color = \"white\";\n",
        "        notification.style.padding = \"10px\";\n",
        "        notification.style.borderRadius = \"5px\";\n",
        "        notification.style.zIndex = 9999;\n",
        "        document.body.appendChild(notification);\n",
        "        setTimeout(() => { notification.remove(); }, 2000);\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"# Image to Title & Keywords\")\n",
        "            image_input = gr.Image(label=\"Upload Image\", type=\"pil\", interactive=True)\n",
        "            submit_btn = gr.Button(\"ðŸš€ Generate\", variant=\"primary\")\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"# Results\")\n",
        "            title_output = gr.Textbox(label=\"Generated Title\", lines=2, interactive=False)\n",
        "            keywords_output = gr.Textbox(label=\"Generated Keywords\", lines=5, interactive=False)\n",
        "            with gr.Row():\n",
        "                copy_title_btn = gr.Button(\"Copy Title\", variant=\"secondary\")\n",
        "                copy_keywords_btn = gr.Button(\"Copy Keywords\", variant=\"secondary\")\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=generate_image_keywords,\n",
        "        inputs=image_input,\n",
        "        outputs=[title_output, keywords_output]\n",
        "    )\n",
        "\n",
        "    copy_title_btn.click(\n",
        "        fn=None,\n",
        "        inputs=title_output,\n",
        "        outputs=[],\n",
        "        js=copy_js\n",
        "    )\n",
        "\n",
        "    copy_keywords_btn.click(\n",
        "        fn=None,\n",
        "        inputs=keywords_output,\n",
        "        outputs=[],\n",
        "        js=copy_js\n",
        "    )\n",
        "\n",
        "demo.queue().launch(share=True, debug=True, inline=False)\n"
      ],
      "metadata": {
        "id": "pAeYlT6DUsbH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
